<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>XaiR - An XR Platform</title>
    <style>
        body {
            font-family: Arial, sans-serif;
        }
        .container {
            width: 80%;
            margin: auto;
        }
        .header {
            text-align: center;
        }
        .authors, .pdf-container {
            display: flex;
            flex-wrap: wrap;
            justify-content: space-between;
        }
        .author, .pdf-container {
            margin-right: 20px;
            margin-bottom: 20px;
        }
        .author b, .pdf-container b {
            display: block;
            margin-bottom: 5px;
        }
        .author small, .pdf-container small {
            display: block;
            margin-bottom: 5px;
        }
        .buttons {
            text-align: center;
            margin-top: 20px;
        }
        .button {
            background-color: black;
            border: none;
            color: white;
            padding: 10px 20px;
            text-align: center;
            text-decoration: none;
            display: inline-block;
            font-size: 16px;
            margin-right: 10px;
            cursor: pointer;
        }
        .button:hover {
            background-color: #333;
        }
        .image {
            text-align: center; /* Center the image */
            margin-top: 20px;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>XaiR</h1>
            <h2>An XR Platform that Integrates Large Language Models with the Physical World</h2>
        </div>
        <br>
        <br>
        <div class="authors">
            <div class="author">
                <b>Sruti Srinidhi</b>
                <small>Carnegie Mellon University</small>
                <a href="mailto:ssrinidh@andrew.cmu.edu">ssrinidh@andrew.cmu.edu</a>
            </div>
            <div class="author">
                <b>Edward Lu</b>
                <small>Carnegie Mellon University</small>
                <a href="mailto:elu2@andrew.cmu.edu">elu2@andrew.cmu.edu</a>
            </div>
            <div class="author">
                <b>Anthony Rowe</b>
                <small>Carnegie Mellon University<br>Bosch Research</small>
                <a href="mailto:agr@andrew.cmu.edu">agr@andrew.cmu.edu</a>
            </div>
        </div>
        <div class="buttons">
            <a href="https://github.com/srutisrinidhi/XaiR" class="button">Paper</a>
            <a href="https://github.com/srutisrinidhi/XaiR" class="button">Code</a>
        </div>
        <br>
        <br>
        <div class="image">
            <img src="https://raw.githubusercontent.com/srutisrinidhi/XaiR/main/docs/high_level_architecture.jpg" alt="Descriptive Image Text" width="100%" />
        </div>
        <br>
        <div class="abstract">
            <p>
                This paper discusses the integration of Multimodal Large Language
                Models (MLLMs) with Extended Reality (XR) headsets, focusing
                on enhancing machine understanding of physical spaces. By combining
                the contextual capabilities of MLLMs with the sensory inputs from XR,
                there is potential for more intuitive spatial interactions. However,
                the integration faces challenges due to the inherent limitations of MLLMs
                in processing 3D inputs and their significant resource demands for XR
                headsets. We introduce XaiR, a platform that facilitates integrating MLLMs
                with XR applications. XaiR uses a split architecture that offloads complex
                MLLM operations to a server while handling 3D world processing on the
                headset. This setup manages multiple input modalities, parallel models,
                and links them with real-time pose data, improving AR content placement
                in physical scenes. We tested XaiR’s effectiveness with a “cognitive
                assistant” application that guides users through tasks like making
                coffee or assembling furniture. Results from a 15-participant study
                show over 90% accuracy in task guidance and 85% accuracy in AR
                content anchoring. Additionally, we evaluate MLLMs against human
                operators for cognitive assistant tasks which provides insights into the
                quality of the captured data as well as the current gap in performance
                for cognitive assistant tasks.
            </p>
        </div>
    </div>
</body>
</html>
